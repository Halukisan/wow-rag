我们计划采用Llama-index来做RAG。Langchain也是个不错的选择。我们之后会加入Langchain的版本，不过目前只做了Llama-index的教程。干活之前我们先准备好必备模型。一个llm模型，和一个embedding模型。

想要借助Llama-index构建llm和embedding模型，我们大体上有三种思路。

第一个思路：使用Llama-index为各个厂家单独构建的服务，比如Llama-index为智谱构建了专门的包，我们可以直接安装使用。
第二个思路：如果Llama-index没有为某个厂家构建的服务，我们可以借助Llama-index为openai构建的库。只要我们国内的模型是openai兼容型的，我们就可以直接使用。
第三个思路：我们可以在本地安装Ollama，在本地安装好模型，然后在Llama-index中使用Ollama的服务。



# 第一个思路，使用Llama-index为智谱构建的专门的包，直接安装最新版本即可。
%pip install llama-index-core
%pip install llama-index-embeddings-zhipuai
%pip install llama-index-llms-zhipuai
%pip install llama-index-readers-file
%pip install llama-index-vector-stores-faiss
%pip install llamaindex-py-client


跟第一节课的开头一样，咱们现在先把四样前菜准备一下吧：
```python
import os
from dotenv import load_dotenv

# 加载环境变量
load_dotenv()
# 从环境变量中读取api_key
api_key = os.getenv('ZHIPU_API_KEY')
base_url = "https://open.bigmodel.cn/api/paas/v4/"
chat_model = "glm-4-flash"
emb_model = "embedding-2"
```


### 配置对话模型
```python
from llama_index.llms.zhipuai import ZhipuAI
llm = ZhipuAI(
    api_key = api_key,
    model = chat_model,
)
```

### 测试对话模型
```python
# 测试对话模型
response = llm.complete("你是谁？")
print(response)
```

### 配置嵌入模型
```python
# 配置嵌入模型
from llama_index.embeddings.zhipuai import ZhipuAIEmbedding
embedding = ZhipuAIEmbedding(
    api_key = api_key,
    model = emb_model,
)
```

### 测试嵌入模型
```python
# 测试嵌入模型
emb = embedding.get_text_embedding("你好呀呀")
len(emb), type(emb)
```
输出 (1024, list)

说明配置成功。



# 第二个思路，借助openai的库，里面接入国内的模型
直接安装最新版本


如果是在jupyter notebook里运行，需要先用魔法命令安装这些库。

%pip install llama-index-core
%pip install llama-index-embeddings-openai
%pip install llama-index-llms-openai
%pip install llama-index-readers-file
%pip install llama-index-vector-stores-faiss
%pip install llamaindex-py-client



我们想要借助llama_index的OpenAI接口使用其他厂家的模型，通过翻阅源码，发现llama index 把OpenAI和OpenAIEmbedding的模型名称写死在代码里了。所以需要修改源码，把我们想要接入的模型名称添加进入，就可以了
假如说我们想要添加进智谱的glm-4-flash和embedding-2模型，实际上其他厂家的模型也是同理。

### 修改对话模型源码
找到这个文件：Lib\site-packages\llama_index\llms\openai\utils.py

在大约第30行，有一个GPT4_MODELS: Dict[str, int]
在这里面列举了很多模型的名称，把智谱的glm-4-flash加进入，变成这样：

"gpt-4": 8192,
"gpt-4-32k": 32768,

改成：

"gpt-4": 8192,
"glm-4-flash": 8192,
"gpt-4-32k": 32768,




### 修改嵌入模型源码

改源码：
Lib\site-packages\llama_index\embeddings\openai\base.py

一共改四个地方

class OpenAIEmbeddingModelType(str, Enum):
最下面增加 
EMBED_2 = "embedding-2"

class OpenAIEmbeddingModeModel(str, Enum):
最下面增加 
EMBED_2 = "embedding-2"

_QUERY_MODE_MODEL_DICT = {
最下面增加 
(OAEM.TEXT_SEARCH_MODE, "embedding-2"): OAEMM.EMBED_2,
    
_TEXT_MODE_MODEL_DICT = {
最下面增加 
(OAEM.TEXT_SEARCH_MODE, "embedding-2"): OAEMM.EMBED_2,

改了上面这四个地方，再调用OpenAIEmbedding这个类，就可以正常使用了
如果怕麻烦，也可以用教程里附的base.py 直接替换。

如果不小心导入过llama_index，改完源码不要忘了重启jupyter内核。

### 运行对话模型
```python
# 从文件导入所需要的secret keys
with open('keys.txt', 'r', encoding='utf-8') as f:  
    keylist = f.read().split('\n')
from llama_index.llms.openai import OpenAI
llm = OpenAI(
    temperature=0.95,
    api_key = api_key,
    model = chat_model,
    api_base = base_url # 注意这里单词不一样
)

# 测试对话模型
response = llm.complete("你是谁？")
print(response)
```


### 运行嵌入模型
```python
# 配置Embedding模型
from llama_index.embeddings.openai import OpenAIEmbedding
embedding = OpenAIEmbedding(
    api_key = api_key,
    model = emb_model,
    api_base = base_url # 注意这里单词不一样
)

emb = embedding.get_text_embedding("你好呀呀")
len(emb), type(emb)
```
输出 (1024, list)

说明配置成功。

# 第三个思路：我们可以在本地安装Ollama

访问 https://ollama.com。 下载Windows版本。直接安装。 安装完成后，打开命令行窗口，输入 ollama，如果出现 Usage: Available Commands: 之类的信息，说明安装成功。
我们用qwen2:7b这个模型就行，整个还不到4G。 运行 ollama run qwen2:7b
如果出现了success，就说明安装成功。 然后会出现一个>>>符号，这就是对话窗口。可以直接输入问题。
想要退出交互页面，直接输入 /bye 就行。斜杠是需要的。否则不是退出交互页面，而是对大模型说话，它会继续跟你聊。
在浏览器中输入 127.0.0.1:11434，如果出现 Ollama is running
说明端口运行正常。

安装完ollama后，我们还需要进行配置一下，主要是两个方面。
第一：这时候模型是放在内存中的。我们希望把模型放在硬盘中。所以，我们可以在硬盘中建一个文件夹，比如： D:\programs\ollama\models
然后新建系统环境变量。 变量名： OLLAMA_MODELS
变量值： D:\programs\ollama\models
第二：这时候的大模型只能通过127.0.0.1:11434来访问。我们希望在局域网中的任何电脑都可以访问。这也是通过新建环境变量来解决。
变量名： OLLAMA_HOST 变量值： 0.0.0.0:11434
这样就完成了配置。是不是非常简单方便？

如果是在jupyter notebook里运行，需要先用魔法命令安装这些库。

%pip install llama-index-core
%pip install llama-index-embeddings-ollama
%pip install llama-index-llms-ollama
%pip install llama-index-readers-file
%pip install llama-index-vector-stores-faiss
%pip install llamaindex-py-client

```python
# 我们先用requets库来测试一下大模型
import json
import requests
# 192.168.0.123就是部署了大模型的电脑的IP，
# 请根据实际情况进行替换
BASE_URL = "http://192.168.0.123:11434/api/chat"
```

直接输出看看：
```python
payload = {
  "model": "qwen2:7b",
  "messages": [
    {
      "role": "user",
      "content": "请写一篇1000字左右的文章，论述法学专业的就业前景。"
    }
  ]
}
response = requests.post(BASE_URL, json=payload)
print(response.text)
```

然后改成流式输出：

```python
payload = {
  "model": "qwen2:7b",
  "messages": [
    {
      "role": "user",
      "content": "请写一篇1000字左右的文章，论述法学专业的就业前景。"
    }
  ],
  "stream": True
}
response = requests.post(BASE_URL, json=payload, stream=True)  # 在这里设置stream=True告诉requests不要立即下载响应内容  
# 检查响应状态码  
if response.status_code == 200:  
    # 使用iter_content()迭代响应体  
    for chunk in response.iter_content(chunk_size=1024):  # 你可以设置chunk_size为你想要的大小  
        if chunk:  
            # 在这里处理chunk（例如，打印、写入文件等）  
            rtn = json.loads(chunk.decode('utf-8')) # 假设响应是文本，并且使用UTF-8编码  
            print(rtn["message"]["content"], end="")
else:  
    print(f"Error: {response.status_code}")  

# 不要忘记关闭响应  
response.close()
```

如果上面输出正常，说明Ollama的配置成功。我们再来配置一下Llama-index。

```python
# 配置chat模型
from llama_index.llms.ollama import Ollama
llm = Ollama(base_url="http://192.168.0.123:11434", model="qwen2:7b")
```

```python
# 测试chat模型
response = llm.complete("你是谁？")
print(response)
# 我是阿里云开发的一款超大规模语言模型，我叫通义千问。
```

```python
# 配置嵌入模型
from llama_index.embeddings.ollama import OllamaEmbedding
embedding = OllamaEmbedding(base_url="http://192.168.0.123:11434", model_name="qwen2:7b")
```

```python
# 测试嵌入模型
emb = embedding.get_text_embedding("你好呀呀")
len(emb), type(emb)
# (3584, list)
```

但是有个问题，就是Ollama这个嵌入模型用来做向量检索，效果不好。一般来说向量的相似度得分是在0到1之间的小数。但是Ollama这个嵌入模型计算出来的得分要么非常大，要么非常小。根据经验，还是智谱的这个向量模型效果最好。