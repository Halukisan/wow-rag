干活之前我们先准备好必备模型。一个llm模型，和一个embedding模型。
Llama index的版本如下：

```python
llama-index-core                         0.10.15
llama-index-embeddings-openai            0.1.5
llama-index-llms-openai                  0.1.8
llama-index-readers-file                 0.1.4
llama-index-vector-stores-elasticsearch  0.1.6
llama-index-vector-stores-faiss          0.1.1
llamaindex-py-client                     0.1.19
```

这里需要注意的是，llama-index-core这个库的版本不能高于0.10.15，llama-index-readers-file这个库的版本不能高于0.1.4，新版本会引入nltk依赖，然后会下载一个语言包，速度超慢。


```python
# 从文件导入所需要的secret keys
with open('keys.txt', 'r', encoding='utf-8') as f:  
    keylist = f.read().split('\n')
from llama_index.llms.openai import OpenAI
llm = OpenAI(
    temperature=0.95,
    model="glm-4-flash",
    api_key=keylist[1],
    api_base="https://open.bigmodel.cn/api/paas/v4/"
)

# 测试对话模型
response = llm.complete("你是谁？")
print(response)
```

对话模型好解决，直接用OpenAI的就行。


嵌入模型有点麻烦，如果直接用OpenAIEmbedding这个类，会报错找不到模型。

通过翻阅源码，发现llama index 把OpenAI的模型名称写死在代码里了。所以需要修改源码，把智谱的模型名称添加进入，就可以了
假如说我们想要添加进智谱的embedding-2模型，实际上embedding-3模型也是同理。

改源码：
Lib\site-packages\llama_index\embeddings\openai\base.py

一共改四个地方

class OpenAIEmbeddingModelType(str, Enum):
最下面增加 
EMBED_2 = "embedding-2"

class OpenAIEmbeddingModeModel(str, Enum):
最下面增加 
EMBED_2 = "embedding-2"

_QUERY_MODE_MODEL_DICT = {
最下面增加 
(OAEM.TEXT_SEARCH_MODE, "embedding-2"): OAEMM.EMBED_2,
    
_TEXT_MODE_MODEL_DICT = {
最下面增加 
(OAEM.TEXT_SEARCH_MODE, "embedding-2"): OAEMM.EMBED_2,

改了上面这四个地方，再调用OpenAIEmbedding这个类，就可以正常使用了
如果怕麻烦，也可以用教程里附的base.py 直接替换。


```python
# 配置Embedding模型
from llama_index.embeddings.openai import OpenAIEmbedding
embedding = OpenAIEmbedding(
    model="embedding-2",
    api_key=keylist[1],
    api_base="https://open.bigmodel.cn/api/paas/v4/"
)

emb = embedding.get_text_embedding("你好呀呀")
len(emb), type(emb)
```
输出 (1024, list)

说明配置成功。

